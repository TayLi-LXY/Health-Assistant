"""
æç®€ç‰ˆæœ¬çš„çŸ¥è¯†åº“æ„å»ºè„šæœ¬
ä½¿ç”¨åŸºäºå­—ç¬¦ä¸²çš„ç®€å•å‘é‡åŒ–æ–¹æ³•ï¼Œé¿å…ç½‘ç»œä¸‹è½½é—®é¢˜
"""

import json
import os
import hashlib
from pathlib import Path
from tqdm import tqdm

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings

VECTOR_STORE_DIR = "knowledge_base/chroma_db_small"


class SimpleEmbeddings(Embeddings):
    """ç®€å•çš„åŸºäºå­—ç¬¦ä¸²å“ˆå¸Œçš„åµŒå…¥æ–¹æ³•"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
    
    def embed_documents(self, texts):
        """ä¸ºæ–‡æ¡£åˆ›å»ºåµŒå…¥"""
        embeddings = []
        for text in texts:
            # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆå›ºå®šé•¿åº¦çš„å‘é‡
            hash_val = hashlib.sha256(text.encode('utf-8')).digest()
            # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸ºæŒ‡å®šç»´åº¦çš„å‘é‡
            vector = []
            for i in range(self.dimension):
                vector.append(float((hash_val[i % len(hash_val)] - 128) / 128))
            embeddings.append(vector)
        return embeddings
    
    def embed_query(self, text):
        """ä¸ºæŸ¥è¯¢åˆ›å»ºåµŒå…¥"""
        return self.embed_documents([text])[0]


def load_processed_chunks():
    """åŠ è½½é¢„å¤„ç†åçš„ chunks"""
    chunks_path = Path("data/processed_kb_chunks.json")
    if not chunks_path.exists():
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°é¢„å¤„ç†æ–‡ä»¶ {chunks_path}")
        return []
    
    print(f"åŠ è½½é¢„å¤„ç†æ–‡ä»¶ï¼š{chunks_path}")
    with open(chunks_path, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    
    print(f"æˆåŠŸåŠ è½½ {len(chunks)} ä¸ª chunks")
    return chunks


def build_vector_store():
    """æ„å»ºå‘é‡åº“"""
    print("å¼€å§‹æ„å»ºå‘é‡åº“...")
    
    # 1. åŠ è½½æ•°æ®
    chunks = load_processed_chunks()
    if not chunks:
        print("é”™è¯¯ï¼šæ²¡æœ‰æ•°æ®å¯å¤„ç†")
        return None
    
    # 2. åˆ›å»ºå‘é‡å­˜å‚¨ç›®å½•
    os.makedirs(VECTOR_STORE_DIR, exist_ok=True)
    
    # 3. åˆå§‹åŒ–ç®€å•åµŒå…¥æ¨¡å‹
    print("åˆå§‹åŒ–ç®€å•åµŒå…¥æ¨¡å‹...")
    embeddings = SimpleEmbeddings(dimension=384)
    print("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
    
    # 4. åˆå§‹åŒ– Chroma
    print(f"åˆå§‹åŒ– Chroma å‘é‡åº“ï¼š{VECTOR_STORE_DIR}")
    vector_store = Chroma(
        persist_directory=VECTOR_STORE_DIR,
        embedding_function=embeddings,
        collection_name="health_kb"
    )
    
    # 5. åˆ†æ‰¹å¤„ç†
    batch_size = 100
    total_chunks = len(chunks)
    print(f"å¼€å§‹æ‰¹é‡æ’å…¥å‘é‡åº“ï¼Œå…± {total_chunks} ä¸ª chunksï¼Œæ‰¹å¤§å°ï¼š{batch_size}")
    
    for i in tqdm(range(0, total_chunks, batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
        batch = chunks[i : i + batch_size]
        
        batch_texts = [c["content"] for c in batch]
        batch_metadatas = []
        
        for c in batch:
            batch_metadatas.append({
                "source_url": c.get("source_url", ""),
                "title": c.get("title", ""),
                "chunk_id": c.get("chunk_id", ""),
                "chunk_index": c.get("chunk_index", 0),
                "document_type": c.get("document_type", "unknown")
            })
        
        try:
            vector_store.add_texts(texts=batch_texts, metadatas=batch_metadatas)
        except Exception as e:
            print(f"æ’å…¥æ‰¹æ¬¡ {i//batch_size + 1} æ—¶å‡ºé”™ï¼š{e}")
            continue
    
    # 6. æŒä¹…åŒ–
    vector_store.persist()
    print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")
    print(f"ğŸ“ å‘é‡åº“ä¿å­˜ä½ç½®ï¼š{VECTOR_STORE_DIR}")
    
    # 7. éªŒè¯
    count = vector_store._collection.count()
    print(f"ğŸ“Š å‘é‡åº“ä¸­åŒ…å« {count} ä¸ªæ–‡æ¡£")
    
    return vector_store


def test_retrieval():
    """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    print("æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embeddings = SimpleEmbeddings(dimension=384)
    
    # åŠ è½½å‘é‡åº“
    vector_store = Chroma(
        persist_directory=VECTOR_STORE_DIR,
        embedding_function=embeddings,
        collection_name="health_kb"
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "é«˜è¡€å‹æ‚£è€…çš„é¥®é£Ÿå»ºè®®",
        "æ„Ÿå†’çš„ç—‡çŠ¶å’Œæ²»ç–—æ–¹æ³•",
        "ç³–å°¿ç—…çš„é¢„é˜²æªæ–½"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢ï¼š{query}")
        results = vector_store.similarity_search(query, k=3)
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼š")
        for i, result in enumerate(results):
            print(f"ç»“æœ {i+1}:")
            print(f"  å†…å®¹: {result.page_content[:100]}...")
            print(f"  æ¥æº: {result.metadata.get('source_url', 'æœªçŸ¥')}")
            print(f"  æ ‡é¢˜: {result.metadata.get('title', 'æœªçŸ¥')}")


def main():
    print("=== æç®€çŸ¥è¯†åº“æ„å»ºè„šæœ¬ ===")
    print("ä½¿ç”¨æœ¬åœ°ç®€å•å‘é‡åŒ–æ–¹æ³•ï¼Œæ— éœ€ç½‘ç»œä¸‹è½½")
    print("=" * 50)
    
    vector_store = build_vector_store()
    
    if vector_store:
        print("\nğŸ‰ æ„å»ºæˆåŠŸï¼")
        print("\næµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        test_retrieval()
        print("\nå¦‚ä½•ä½¿ç”¨çŸ¥è¯†åº“ï¼š")
        print("1. å¯åŠ¨åç«¯æœåŠ¡ï¼špython -m uvicorn main:app --reload")
        print("2. è®¿é—®å‰ç«¯ï¼šhttp://localhost:5173")
        print("3. æµ‹è¯•æ£€ç´¢åŠŸèƒ½ï¼šåœ¨å‰ç«¯è¾“å…¥å¥åº·é—®é¢˜")
    else:
        print("\nâŒ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()
